from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import StaleElementReferenceException
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
from datetime import datetime
from datetime import datetime, timedelta
from fake_useragent import UserAgent

import os
import json
import requests
import re
import time 
import math
import sys


sys.stdout.reconfigure(encoding='utf-8')
# # Chrome ë“œë¼ì´ë²„ ì˜µì…˜ ì„¤ì •
options = Options()
options.add_argument("--headless")  # í™”ë©´ ì—†ì´ ì‹¤í–‰
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
ua = UserAgent()

# random user-agent ì„¤ì •
def get_pc_user_agent():
    while True:
        candidate = ua.random
        if not re.search(r'Mobile|Android|iPhone', candidate, re.I):
            return candidate
user_agent = get_pc_user_agent()
options.add_argument(f'user-agent={user_agent}')



# service = Service("/path/to/chromedriver")  # chromedriver ê²½ë¡œ
driver = webdriver.Chrome(options)

def is_toplist_open(driver):
    try:
        wrapper = driver.find_element(By.ID, "toplistWrapper")

        # display, height, aria ìƒíƒœ ê°ì§€
        display = driver.execute_script("return window.getComputedStyle(arguments[0]).display;", wrapper)
        height = driver.execute_script("return arguments[0].offsetHeight;", wrapper)
        aria_hidden = wrapper.get_attribute("aria-hidden") or wrapper.get_attribute("area-hidden")

        visible = wrapper.is_displayed()
        aria_open = (aria_hidden is None) or (aria_hidden.lower() == "false")

        # ğŸ”§ ì™„í™”ëœ ì¡°ê±´: display != none ë˜ëŠ” height > 50
        return (display != "none" or height > 50) and aria_open and visible

    except Exception:
        return False


from selenium.common.exceptions import StaleElementReferenceException, TimeoutException

def ensure_toplist_open(driver, timeout=5):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ Toplist(ëª©ë¡)ê°€ ë‹«í˜€ ìˆì„ ê²½ìš° ì—´ë¦¬ë„ë¡ ë³´ì¥í•œë‹¤.
    StaleElementReferenceException ë°œìƒ ì‹œ ì¬ì‹œë„í•˜ë„ë¡ ìˆ˜ì •.
    """
    try:
        # ì´ë¯¸ í”„ë ˆì„ ì•ˆì´ë¼ë©´ ìƒëµ
        try:
            driver.find_element(By.ID, "toplistWrapper")
        except:
            switched = switch_to_main_frame(driver)
            if not switched:
                print(" mainFrame ì „í™˜ ì‹¤íŒ¨ â†’ toplist ì—´ê¸° ê±´ë„ˆëœ€")
                return

        span_elem = WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.ID, "toplistSpanBlind"))
        )
        span_text = span_elem.text.strip()
        print(f"í˜„ì¬ ëª©ë¡ ìƒíƒœ í…ìŠ¤íŠ¸: {span_text}")

        if "ëª©ë¡ë‹«ê¸°" in span_text:
            print(" ëª©ë¡ì´ ì´ë¯¸ ì—´ë ¤ ìˆìŒ â†’ í´ë¦­ ìƒëµ")
            return

        print("ëª©ë¡ì´ ë‹«í˜€ ìˆìŒ â†’ ì—´ê¸° ì‹œë„")

        # 1ì´ˆ ëŒ€ê¸° (ë„¤ì´ë²„ ë‚´ë¶€ JS í•¸ë“¤ëŸ¬ ë¡œë“œ ëŒ€ê¸°)
        time.sleep(1)

        # ìµœëŒ€ 3ë²ˆ ì¬ì‹œë„
        for attempt in range(3):
            try:
                toggle_btn = WebDriverWait(driver, timeout).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "a.btn_openlist._toggleTopList"))
                )

                driver.execute_script("""
                    const el = arguments[0];
                    el.scrollIntoView({behavior:'instant', block:'center'});
                    const evt = new MouseEvent('click', {bubbles:true, cancelable:true, view:window});
                    el.dispatchEvent(evt);
                """, toggle_btn)

                print(f"í´ë¦­ ì‹œë„ {attempt+1}íšŒ ì™„ë£Œ")

                # display ìƒíƒœê°€ blockìœ¼ë¡œ ë°”ë€” ë•Œê¹Œì§€ ëŒ€ê¸°
                WebDriverWait(driver, timeout * 3).until(
                    lambda d: d.execute_script(
                        "return window.getComputedStyle(document.querySelector('#toplistWrapper')).display"
                    ) != "none"
                )
                print(" ëª©ë¡ ì—´ë¦¼ í™•ì¸ ì™„ë£Œ")
                return
            except StaleElementReferenceException:
                print(f" StaleElementReference ë°œìƒ â†’ {attempt+1}ë²ˆì§¸ ì¬ì‹œë„")
                time.sleep(0.5)
                continue

        print(" 3íšŒ ì¬ì‹œë„ í›„ ì‹¤íŒ¨")

    except TimeoutException:
        print(" Timeout: ëª©ë¡ì´ ì—´ë¦¬ì§€ ì•ŠìŒ (ì´ë¯¸ ì—´ë ¤ ìˆê±°ë‚˜ í† ê¸€ ì‹¤íŒ¨)")

def switch_to_main_frame(driver, timeout=10):
    """
    mainFrameì„ ì•ˆì „í•˜ê²Œ ì „í™˜í•˜ëŠ” í•¨ìˆ˜.
    ID ë˜ëŠ” NAME ê¸°ì¤€ìœ¼ë¡œ ëª¨ë‘ íƒìƒ‰, ì—†ì„ ê²½ìš° ìŠ¤í‚µ.
    """
    try:
        WebDriverWait(driver, timeout).until(
            EC.any_of(
                EC.frame_to_be_available_and_switch_to_it((By.ID, "mainFrame")),
                EC.frame_to_be_available_and_switch_to_it((By.NAME, "mainFrame"))
            )
        )
        print(" mainFrame ì „í™˜ ì„±ê³µ")
        return True
    except TimeoutException:
        print(" mainFrameì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ URL:", driver.current_url)
        # ë””ë²„ê¹…ìš© HTML ì €ì¥
        with open("no_frame_debug.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        return False
    
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LATEST_POST_DIR = os.path.join(BASE_DIR, "latest_post")

def save_last_url(blog_url, url):
    """
    ê° ë¸”ë¡œê·¸ ì£¼ì†Œë³„ë¡œ ìµœì‹  í¬ìŠ¤íŠ¸ URLì„ ì €ì¥.
    ì˜ˆ: latest_post/kse4966.json
    """
    # ë¸”ë¡œê·¸ ID ì¶”ì¶œ
    match = re.search(r'blog.naver.com/([^/?]+)', blog_url)
    blog_id = match.group(1) if match else "unknown"

    # ì €ì¥ í´ë” ìƒì„± (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
    folder = "latest_post"
    os.makedirs(folder, exist_ok=True)

    # íŒŒì¼ ê²½ë¡œ
    filepath = os.path.join(folder, f"{blog_id}.json")

    # ë°ì´í„° ì €ì¥
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump({"latest_post_url": url}, f, ensure_ascii=False, indent=2)

    print(f" ìµœì‹  í¬ìŠ¤íŠ¸ URL ì €ì¥ ì™„ë£Œ â†’ {filepath}")


import os
import json
import re

def find_project_root(target_folder_name="brich-project"):

    current_dir = os.path.abspath(__file__)
    while True:
        current_dir = os.path.dirname(current_dir)
        if os.path.basename(current_dir) == target_folder_name:
            return current_dir
        if current_dir == os.path.dirname(current_dir):  # ë£¨íŠ¸ ë„ë‹¬
            break
    return None

def load_last_url(blog_url):

    #  ë£¨íŠ¸ ìë™ íƒìƒ‰
    root_dir = find_project_root("brich-project")
    if not root_dir:
        print("í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    folder = os.path.join(root_dir, "latest_post")

    # ë¸”ë¡œê·¸ ID ì¶”ì¶œ
    match = re.search(r'blog.naver.com/([^/?]+)', blog_url)
    blog_id = match.group(1) if match else "unknown"

    filepath = os.path.join(folder, f"{blog_id}.json")

    # íŒŒì¼ í™•ì¸
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            print(f"ë¶ˆëŸ¬ì˜¨ ê²½ë¡œ: {filepath}")
            return data.get("latest_post_url", None)
    else:
        print(f" {filepath} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

from urllib.parse import urlparse, parse_qs

def normalize_post_url(url: str) -> str:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ URLì—ì„œ blogIdì™€ logNoë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
    ì˜ˆ: https://blog.naver.com/PostView.naver?blogId=abc&logNo=123&categoryNo=0 -> 
        https://blog.naver.com/PostView.naver?blogId=abc&logNo=123
    """
    try:
        if not url.startswith("http"):
            return url  # ì ˆëŒ€ê²½ë¡œ ì•„ë‹ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜

        parsed = urlparse(url)
        query = parse_qs(parsed.query)

        blog_id = query.get("blogId", [None])[0]
        log_no = query.get("logNo", [None])[0]

        if blog_id and log_no:
            return f"https://blog.naver.com/PostView.naver?blogId={blog_id}&logNo={log_no}"
        else:
            return url
    except Exception as e:
        print(f"URL ì •ê·œí™” ì‹¤íŒ¨: {url} ({e})")
        return url


# ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì§„ì…
blg_url = sys.argv[1]
driver.get(blg_url) # ë¸”ë¡œê·¸ IDë¥¼ ì¸ìë¡œ ë°›ìŒ
wait = WebDriverWait(driver, 30)

# 1. iframeì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° í›„ ì „í™˜
if not switch_to_main_frame(driver):
    print(" mainFrameì´ ì—†ëŠ” í˜ì´ì§€ë¡œ íŒë‹¨ë˜ì–´ í”„ë ˆì„ ì „í™˜ ìƒëµ")


blog_link = WebDriverWait(driver, 3).until(
    EC.element_to_be_clickable((By.XPATH, "//a[contains(@href, 'PostList.naver') and contains(@class, 'itemfont') and contains(@class, '_doNclick') and contains(@class, '_param(false|blog|)')]"))
    ,print("ë¸”ë¡œê·¸ íƒ­ í´ë¦­")
)
blog_link.click()

# ì „ì²´ë³´ê¸° í´ë¦­
try:
    all_posts_link = WebDriverWait(driver, 5).until(
        EC.element_to_be_clickable((By.XPATH, "//a[@id='category0' and contains(text(), 'ì „ì²´ë³´ê¸°')]"))
    )
    driver.execute_script("arguments[0].click();", all_posts_link)
    print("ì „ì²´ë³´ê¸° í´ë¦­ ì™„ë£Œ")
except TimeoutException:
    print("ì „ì²´ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")




#  ëª©ë¡ ì—´ë¦¼ ìƒíƒœ ë³´ì¥
ensure_toplist_open(driver)

# 2 ì ê¹ ëŒ€ê¸° (ëª©ë¡ DOM ì™„ì „íˆ ê°±ì‹ ë  ë•Œê¹Œì§€)
time.sleep(0.5)

# 3 ëª©ë¡ì´ ì—´ë¦° ìƒíƒœì˜ HTMLë¡œ ìƒˆë¡œ íŒŒì‹±
soup = BeautifulSoup(driver.page_source, 'html.parser')

# 4 í˜ì´ì§€ ê°œìˆ˜ ì¶”ì¶œ
page_count_elem = soup.select_one('h4.category_title.pcol2')
numeric_chars = [char for char in page_count_elem.text if char.isdigit()]
numeric_string = "".join(numeric_chars)


# list_size = soup.select_one('#listCountView').text
# list_size = re.findall(r'\d+', list_size)[0]
last_url = load_last_url(blg_url)
stop_collecting = False
links = []  # setìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€
seen = set()
total_pages = math.ceil(int(numeric_string) / 5)



for page_num in range(1, 30):
    # í˜„ì¬ í˜ì´ì§€ HTML ìƒˆë¡œ íŒŒì‹±
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # ë§í¬ ìˆ˜ì§‘ (ì ˆëŒ€ê²½ë¡œ + ì •í™•í•œ í´ë˜ìŠ¤ í•„í„°)
    for a in soup.find_all('a', href=True):
        href = a['href']
        href = normalize_post_url(href)
        classes = a.get('class', [])
        if (
            href.startswith('https://blog.naver.com/PostView.naver?blogId=') and
            all(c in classes for c in ['pcol2', '_setTop', '_setTopListUrl']) and
            not a.has_attr('logno') and
            not a.has_attr('onclick') and
            href not in seen
        ):
            if last_url and href == last_url:
                print(f" ë§ˆì§€ë§‰ ìˆ˜ì§‘ í¬ìŠ¤íŠ¸ ë„ë‹¬: {href} â†’ í¬ë¡¤ë§ ì¤‘ë‹¨")
                stop_collecting = True
                break
            
            links.append(href)  
            seen.add(href) # setì´ë¼ ì¤‘ë³µ ì•ˆ ë¨

    if stop_collecting:
        break
    print(f"[PAGE {page_num}] ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(links)}")

    # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
    next_xpath = f"//a[contains(@class,'_goPageTop') and contains(@class,'_param({page_num+1})')]"
    try:
        next_button = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.XPATH, next_xpath))
        )
        driver.execute_script("arguments[0].click();", next_button)
        
        # í˜ì´ì§€ê°€ ì‹¤ì œë¡œ ë°”ë€” ë•Œê¹Œì§€ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(EC.staleness_of(next_button))
        WebDriverWait(driver, 2).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.blog2_list"))
        )
    except TimeoutException:
        print(f"í˜ì´ì§€ {page_num}ì—ì„œ ë‹¤ìŒ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë§ˆì§€ë§‰ í˜ì´ì§€ì¼ ìˆ˜ ìˆìŒ).")
        break




print(f"ì´ ê³ ìœ  ë§í¬ ìˆ˜: {len(links)}")
for l in sorted(links):
    print(l)
   

# í´ë¦­ í›„ì˜ HTML ê°€ì ¸ì˜¤ê¸°
html = driver.page_source
print(html[:1000])  # ì•ë¶€ë¶„ë§Œ ì¶œë ¥í•´ë³´ê¸°

count = 1
results = []

for idx, post_url in enumerate(links):
    driver.get(post_url)
    WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, 'iframe')))
    try:
        iframe = driver.find_element(By.ID, "mainFrame")
        driver.switch_to.frame(iframe)
    except Exception:
        pass
    
    # í”„ë ˆì„ ì „í™˜ í›„
    WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")

    soup = BeautifulSoup(driver.page_source, 'html.parser')

    like_elem = soup.select_one('span.u_likeit_text._count.num')
    # ê³µê° ìˆ˜ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ë‹¤ì‹œ ì‹œë„
    if not like_elem or not like_elem.text.strip():
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        like_elem = soup.select_one('span.u_likeit_text._count.num')

    like_count = like_elem.text.strip() if like_elem and like_elem.text.strip() else 'N/A'

    # ë‚ ì§œ
    post_elem = soup.select_one('span.se_publishDate.pcol2')
    post_date = post_elem.text.strip() if post_elem else 'N/A'
    
    now = datetime.now()
    # ====== ë‚ ì§œ íŒŒì‹± ======
    if re.search(r'(ì‹œê°„|ë¶„|ì¼)\s*ì „', post_date):
        print(f"ìµœê·¼ 5ì¼ ì´ë‚´ í¬ìŠ¤íŠ¸ â†’ ì œì™¸: {post_date}")
        continue  # ìµœê·¼ 5ì¼ ì´ë‚´ í¬ìŠ¤íŠ¸ ì œì™¸
    else:
        try:
            post_datetime = datetime.strptime(post_date, "%Y. %m. %d. %H:%M")
            if now - post_datetime < timedelta(days=5):
                print(f"ìµœê·¼ 5ì¼ ì´ë‚´ í¬ìŠ¤íŠ¸ â†’ ì œì™¸: {post_date}")
                continue
        except ValueError:
            try:
                post_datetime = datetime.strptime(post_date, "%Y. %m. %d.")
            except ValueError:
                print(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {post_date}")
                continue

    # ====== ëŒ“ê¸€ ======
    comment_elem = soup.select_one('em._commentCount')
    comment_count = comment_elem.text.strip() if comment_elem else 'N/A'

    results.append({
        'index': idx,
        'url': post_url,
        'date': post_datetime,
        'likes': like_count,
        'comments': comment_count
    })

    print(f"{post_url} | ë‚ ì§œ: {post_datetime} | ê³µê°: {like_count} | ëŒ“ê¸€: {comment_count} | ì¸ë±ìŠ¤: {idx+1}")


# ====== ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ ======
# results.sort(key=lambda x: x["date"], reverse=False)

if results:
    # ê²°ê³¼ ì¤‘ ê°€ì¥ ìµœì‹ (ë‚ ì§œê°€ ê°€ì¥ í°) í¬ìŠ¤íŠ¸ ì°¾ê¸°
    newest = max(results, key=lambda r: r["date"])

    # ë¸”ë¡œê·¸ë³„ ìµœì‹  í¬ìŠ¤íŠ¸ URL ì €ì¥ (ì¸ì ìˆ˜ì •)
    save_last_url(blg_url, newest["url"])

    print(f"ìµœì‹  í¬ìŠ¤íŠ¸ URL ì €ì¥ ì™„ë£Œ â†’ {newest['url']}")
else:
    print("ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ 5ì¼ ì´ë‚´ ë˜ëŠ” ìˆ˜ì§‘ ì‹¤íŒ¨)")

# ====== API ì „ì†¡ ======
url = "http://localhost:8080/api/results"

def safe_int(value):
    try:
        return int(value)
    except:
        return 0
data = {
    "blgAddrs": blg_url,   
    "postList": [           
        {
            "pstUrl": r["url"],
            "pstCmnt": safe_int(r["comments"]),
            "pstLk": safe_int(r["likes"]),
            "pstdDt": r["date"].strftime("%Y-%m-%d %H:%M")
        }
        for r in results
    ]
}
print("=== ì „ì†¡ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===")
print(json.dumps(data, indent=2, ensure_ascii=False))
print("==========================")

response = requests.post(
    url,
    headers={"Content-Type": "application/json"},
    json=data  
)
driver.quit()
